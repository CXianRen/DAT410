{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Aassigment 5 Machine Translation: IBM Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: [('the', 19627), ('of', 9534), ('to', 8992), ('and', 7214), ('in', 6197), ('is', 4453), ('that', 4421), ('a', 4388), ('we', 3341), ('this', 3332)]\n",
      "French: [('apos', 16729), ('de', 14528), ('la', 9746), ('et', 6620), ('l', 6536), ('le', 6177), ('Ã ', 5588), ('les', 5587), ('des', 5232), ('que', 4797)]\n",
      "English:0.004571%\n",
      "English:0.000000%\n"
     ]
    }
   ],
   "source": [
    "# write code to collect statistics about word frequencies in the two languages. \n",
    "# Print the 10 most frequent words in each language.\n",
    "# using a Counter\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "source=\"./dat410_europarl/europarl-v7.fr-en.lc.en\"\n",
    "target=\"./dat410_europarl/europarl-v7.fr-en.lc.fr\"\n",
    "\n",
    "def get_words(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        text = f.read()\n",
    "    words = re.findall(r'\\w+', text.lower())\n",
    "    return words\n",
    "\n",
    "def get_10_most_frequent_words():\n",
    "    english_words = get_words(source)\n",
    "    french_words = get_words(target)\n",
    "    english_word_counts = Counter(english_words)\n",
    "    french_word_counts = Counter(french_words)\n",
    "    print('English:', english_word_counts.most_common(10))\n",
    "    print('French:', french_word_counts.most_common(10))\n",
    "\n",
    "get_10_most_frequent_words()\n",
    "\n",
    "# print the probabitly of \"zebra\" in the two languages\n",
    "english_words = get_words(source)\n",
    "english_word_counts = Counter(english_words)\n",
    "print(\"English:%f%%\" % (english_word_counts['speaker'] / len(english_words) * 100)) \n",
    "print(\"English:%f%%\" % (english_word_counts['zebra'] / len(english_words) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Normal case\n",
      "P(house|the) is 0.002139909308605 \t P(S): 0.002139909308605\n",
      "P(rose|house) is 0.006329113924051 \t P(S): 0.000013543729801\n",
      "P('the house rose') is 0.000013543729801, lP: 0.000013543729801 \n",
      "-----\n",
      "P(says|it) is 0.001999200319872 \t P(S): 0.001999200319872\n",
      "P(that|says) is 0.529411764705882 \t P(S): 0.001058400169344\n",
      "P(this|that) is 0.046821985976024 \t P(S): 0.000049556397886\n",
      "P(should|this) is 0.003901560624250 \t P(S): 0.000000193347291\n",
      "P(be|should) is 0.295402298850575 \t P(S): 0.000000057115234\n",
      "P(done|be) is 0.016312594840668 \t P(S): 0.000000000931698\n",
      "P('it says that this should be done') is 0.000000000931698, lP: 0.000000000931698 \n",
      "----- Non exist\n",
      "P(house|the) is 0.002139909308605 \t P(S): 0.002139909308605\n",
      "P(rose|house) is 0.006329113924051 \t P(S): 0.000013543729801\n",
      "P(NNN|rose) is 0.000003808972415 \t P(S): 0.000000000051588\n",
      "P('the house rose NNN') is 0.000000000051588, lP: 0.000000000051588 \n",
      "----- over length\n",
      "P(will|you) is 0.052256532066508 \t P(S): 0.052256532066508\n",
      "P(be|will) is 0.251017639077341 \t P(S): 0.013117311305704\n",
      "P(aware|be) is 0.004172989377845 \t P(S): 0.000054738400745\n",
      "P(from|aware) is 0.025974025974026 \t P(S): 0.000001421776643\n",
      "P(the|from) is 0.373546511627907 \t P(S): 0.000000531099705\n",
      "P(press|the) is 0.000509502216335 \t P(S): 0.000000000270596\n",
      "P(and|press) is 0.150000000000000 \t P(S): 0.000000000040589\n",
      "P(television|and) is 0.000138619351261 \t P(S): 0.000000000000006\n",
      "P(that|television) is 0.125000000000000 \t P(S): 0.000000000000001\n",
      "P(there|that) is 0.028726532458720 \t P(S): 0.000000000000000\n",
      "P(have|there) is 0.013647642679901 \t P(S): 0.000000000000000\n",
      "P(been|have) is 0.115221987315011 \t P(S): 0.000000000000000\n",
      "P(a|been) is 0.029914529914530 \t P(S): 0.000000000000000\n",
      "P(number|a) is 0.016180492251595 \t P(S): 0.000000000000000\n",
      "P(of|number) is 0.924657534246575 \t P(S): 0.000000000000000\n",
      "P(bomb|of) is 0.000104887770086 \t P(S): 0.000000000000000\n",
      "P(explosions|bomb) is 0.200000000000000 \t P(S): 0.000000000000000\n",
      "P(and|explosions) is 1.000000000000000 \t P(S): 0.000000000000000\n",
      "P(killings|and) is 0.000138619351261 \t P(S): 0.000000000000000\n",
      "P(in|killings) is 0.500000000000000 \t P(S): 0.000000000000000\n",
      "P(sri|in) is 0.000484105212199 \t P(S): 0.000000000000000\n",
      "P(lanka|sri) is 0.750000000000000 \t P(S): 0.000000000000000\n",
      "len(122) ,P('you will b ...') is 0.000000000000000, lP: 0.000000000000000 \n"
     ]
    }
   ],
   "source": [
    "# bigram language model\n",
    "\n",
    "from math import log,exp\n",
    "\n",
    "class BigramLanguageModel:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def train(self, words):\n",
    "        self.bigrams = Counter(zip(words, words[1:]))\n",
    "        self.unigrams = Counter(words)\n",
    "        self.total = len(words)\n",
    "        \n",
    "    def probability_w(self, word1, word2):\n",
    "        if self.unigrams[word1] == 0 \\\n",
    "            or self.unigrams[word2] ==0 \\\n",
    "            or self.bigrams[(word1,word2)] == 0:\n",
    "            # if word non exist\n",
    "            return 1.0 / (self.total + 1)\n",
    "        return self.bigrams[(word1,word2)]/self.unigrams[word1]\n",
    "    \n",
    "    def predict(self, words: list):\n",
    "        # length of words should be at least 1\n",
    "        if len(words) == 0: return 1\n",
    "        Ps=1.0 # the probability_w (words[0]|<start>) is 1\n",
    "        for i in range(1,len(words)):\n",
    "            pt = self.probability_w(words[i-1],words[i])\n",
    "            Ps = Ps* pt\n",
    "            print(\"P(%s|%s) is %.15f \\t P(S): %.15f\" %(words[i],words[i-1], pt, Ps))\n",
    "            # print(\"\\t (%s, %s):%d, (%s): %d\" %(words[i-1],words[i],\n",
    "            #                                    self.bigrams[(words[i-1],words[i])], \n",
    "            #                                    words[i-1],\n",
    "            #                                    self.unigrams[words[i-1]]))\n",
    "        return Ps\n",
    "    \n",
    "    def predict_log(self, words: list):\n",
    "        # length of words should be at least 1\n",
    "        if len(words) == 0: return 1\n",
    "        Ps=0.0 # the probability_w (words[0]|<start>) is 1\n",
    "        for i in range(1,len(words)):\n",
    "            pt = log(self.probability_w(words[i-1],words[i]))\n",
    "            Ps = Ps + pt\n",
    "            # print(\"P(%s|%s) is %.15f \\t P(S): %.15f\" %(words[i],words[i-1], pt, Ps))\n",
    "            # print(\"\\t (%s, %s):%d, (%s): %d\" %(words[i-1],words[i],\n",
    "            #                                    self.bigrams[(words[i-1],words[i])], \n",
    "            #                                    words[i-1],\n",
    "            #                                    self.unigrams[words[i-1]]))\n",
    "        return exp(Ps)\n",
    "\n",
    "En_bigramM = BigramLanguageModel()\n",
    "En_bigramM.train(english_words)\n",
    "\n",
    "\n",
    "\n",
    "# print(english_words[:100])\n",
    "\n",
    "# debug\n",
    "# En_bigamM.probability_w('please','open') \n",
    "# En_bigamM.bigrams.most_common(10)\n",
    "# \n",
    "\n",
    "# case 1 normal short sentence \"the house rose\", from dataset\n",
    "print(\"----- Normal case\")\n",
    "sentence = \"the house rose\"\n",
    "print(\"P('%s') is %.15f, lP: %.15f \" % (sentence, \n",
    "                En_bigramM.predict(sentence.split(' ')),\n",
    "                En_bigramM.predict_log(sentence.split(' '))))\n",
    "print(\"-----\")\n",
    "sentence = \"it says that this should be done\"\n",
    "print(\"P('%s') is %.15f, lP: %.15f \" % (sentence, \n",
    "                En_bigramM.predict(sentence.split(' ')),\n",
    "                En_bigramM.predict_log(sentence.split(' '))))\n",
    "\n",
    "# case 2 words non exist\n",
    "print(\"----- Non exist\")\n",
    "sentence = \"the house rose NNN\"\n",
    "print(\"P('%s') is %.15f, lP: %.15f \" % (sentence, \n",
    "                En_bigramM.predict(sentence.split(' ')),\n",
    "                En_bigramM.predict_log(sentence.split(' '))))\n",
    "\n",
    "# case 3 words super length\n",
    "print(\"----- over length\")\n",
    "sentence = \"you will be aware from the press and television that there have been a number of bomb explosions and killings in sri lanka\"\n",
    "print(\"len(%d) ,P('%s ...') is %.15f, lP: %.15f \" % (len(sentence),\n",
    "                sentence[:10],\n",
    "                En_bigramM.predict(sentence.split(' ')),\n",
    "                En_bigramM.predict_log(sentence.split(' '))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(source_sentences): 10000, len(target_sentences): 10000\n",
      "Initializing t\n",
      "len(f_words): 10000, len(e_words): 10000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/chalmers/users/chenhon/DAT410/Assignment/A5/code/main.ipynb Cell 4\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/chalmers/users/chenhon/DAT410/Assignment/A5/code/main.ipynb#W3sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m ibm1 \u001b[39m=\u001b[39m IBM1()\n\u001b[1;32m     <a href='vscode-notebook-cell:/chalmers/users/chenhon/DAT410/Assignment/A5/code/main.ipynb#W3sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m \u001b[39m# how many iteration we should run?\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/chalmers/users/chenhon/DAT410/Assignment/A5/code/main.ipynb#W3sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m ibm1\u001b[39m.\u001b[39;49mtrain(source_sentences, target_sentences, n_iter\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/chalmers/users/chenhon/DAT410/Assignment/A5/code/main.ipynb#W3sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m \u001b[39mprint\u001b[39m(ibm1\u001b[39m.\u001b[39mt[(\u001b[39m'\u001b[39m\u001b[39mthe\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mle\u001b[39m\u001b[39m'\u001b[39m)])\n",
      "\u001b[1;32m/chalmers/users/chenhon/DAT410/Assignment/A5/code/main.ipynb Cell 4\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/chalmers/users/chenhon/DAT410/Assignment/A5/code/main.ipynb#W3sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mt[(e,f)] \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/chalmers/users/chenhon/DAT410/Assignment/A5/code/main.ipynb#W3sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m         c_e_f[(e,f)] \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m    \n\u001b[0;32m---> <a href='vscode-notebook-cell:/chalmers/users/chenhon/DAT410/Assignment/A5/code/main.ipynb#W3sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m         \u001b[39mif\u001b[39;00m f \u001b[39m==\u001b[39m\u001b[39m'\u001b[39m\u001b[39mla\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/chalmers/users/chenhon/DAT410/Assignment/A5/code/main.ipynb#W3sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m             \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mt(\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m|\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m) = \u001b[39m\u001b[39m%.15f\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (e,f, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mt[(e,f)]))\n\u001b[1;32m     <a href='vscode-notebook-cell:/chalmers/users/chenhon/DAT410/Assignment/A5/code/main.ipynb#W3sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m# start iteration\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# translate model IBM1\n",
    "\n",
    "class IBM1():\n",
    "    def __init__(self):\n",
    "        self.t = {}\n",
    "    \n",
    "    def train(self, source_sentences, target_sentences, n_iter=5):\n",
    "        # generate and initialize t(e|f), c(e -> f), c(e) uniformly\n",
    "        c_e_f={}\n",
    "        c_e={}\n",
    "        print(\"Initializing t\")\n",
    "        f_words = set()\n",
    "        e_words = set()\n",
    "        for k in range(len(source_sentences)):\n",
    "            f_sentences = source_sentences[k]\n",
    "            e_sentences = target_sentences[k]\n",
    "            f_words.add(word for word in re.findall(r'\\w+', f_sentences))\n",
    "            e_words.add(word for word in re.findall(r'\\w+', f_sentences))\n",
    "            print(f_words)\n",
    "        for e in e_words:\n",
    "            c_e[e] = 0.0\n",
    "        for f in f_words:\n",
    "            for e in e_words:\n",
    "                self.t[(e,f)] = 1.0\n",
    "                c_e_f[(e,f)] = 0.0    \n",
    "                if f =='la':\n",
    "                    print(\"t(%s|%s) = %.15f\" % (e,f, self.t[(e,f)]))\n",
    "                    \n",
    "        # start iteration\n",
    "        print(\"Start iteration\")\n",
    "        def single_iteration():\n",
    "            # set c(e -> f) and c(e) to 0\n",
    "            for key in c_e.keys():\n",
    "                c_e[key] = 0.0\n",
    "            for key in c_e_f.keys():\n",
    "                c_e_f[key] = 0.0\n",
    "            # update c(e -> f) and c(e)\n",
    "            for k in range(len(source_sentences)):\n",
    "                f_sentences = source_sentences[k]\n",
    "                e_sentences = target_sentences[k]\n",
    "                \n",
    "                f_words = re.findall(r'\\w+', f_sentences)\n",
    "                e_words = re.findall(r'\\w+', e_sentences)\n",
    "                \n",
    "                for i in range(len(f_words)):\n",
    "                    f = f_words[i]\n",
    "                    esum = sum([self.t[(ej,f)] for ej in e_words])\n",
    "                    for j in range(len(e_words)):\n",
    "                        e = e_words[j]\n",
    "                        theta = self.t[(e,f)]/esum\n",
    "                        c_e_f[(e,f)] += theta\n",
    "                        c_e[e] += theta\n",
    "            # update t(e|f)\n",
    "            for k in range(len(source_sentences)):\n",
    "                f_sentences = source_sentences[k]\n",
    "                e_sentences = target_sentences[k]\n",
    "                \n",
    "                f_words = re.findall(r'\\w+', f_sentences)\n",
    "                e_words = re.findall(r'\\w+', e_sentences)\n",
    "                \n",
    "                for f in f_words:\n",
    "                    for e in e_words:\n",
    "                        self.t[(e,f)] = c_e_f[(e,f)]/c_e[e]\n",
    "                        \n",
    "        for i in range(n_iter):\n",
    "            single_iteration()\n",
    "            print(\"iteration %d\" % i)         \n",
    "        \n",
    "                            \n",
    "        \n",
    "\n",
    "source_sentences = open(source, 'r').readlines()\n",
    "target_sentences = open(target, 'r').readlines()\n",
    "print(\"len(source_sentences): %d, len(target_sentences): %d\" % (len(source_sentences), len(target_sentences)))\n",
    "\n",
    "ibm1 = IBM1()\n",
    "# how many iteration we should run?\n",
    "ibm1.train(source_sentences, target_sentences, n_iter=1)\n",
    "print(ibm1.t[('the','le')])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
