{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Aassigment 5 Machine Translation: IBM Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: [('the', 19627), ('of', 9534), ('to', 8992), ('and', 7214), ('in', 6197), ('is', 4453), ('that', 4421), ('a', 4388), ('we', 3341), ('this', 3332)]\n",
      "X: [('apos', 16729), ('de', 14528), ('la', 9746), ('et', 6620), ('l', 6536), ('le', 6177), ('à', 5588), ('les', 5587), ('des', 5232), ('que', 4797)]\n",
      "English:0.004570784308497%\n",
      "English:0.000000000000000%\n"
     ]
    }
   ],
   "source": [
    "# write code to collect statistics about word frequencies in the two languages. \n",
    "# Print the 10 most frequent words in each language.\n",
    "# using a Counter\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "source=\"./dat410_europarl/europarl-v7.fr-en.lc.fr\"\n",
    "target=\"./dat410_europarl/europarl-v7.fr-en.lc.en\"\n",
    "\n",
    "def get_words(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        text = f.read()\n",
    "    # words = clean_text(text).split(' ')\n",
    "    return re.findall(r'\\b\\w+\\b', text)\n",
    "\n",
    "def get_10_most_frequent_words():\n",
    "    english_words = get_words(target)\n",
    "    french_words = get_words(source)\n",
    "    english_word_counts = Counter(english_words)\n",
    "    french_word_counts = Counter(french_words)\n",
    "    ten_most_common_english = english_word_counts.most_common(10)\n",
    "    ten_most_common_french = french_word_counts.most_common(10)\n",
    "    print('English:', ten_most_common_english)\n",
    "    print('X:', ten_most_common_french)\n",
    "    return ten_most_common_english, ten_most_common_french\n",
    "    \n",
    "ten_most_common_english, ten_most_common_french = get_10_most_frequent_words()\n",
    "\n",
    "# print the probabitly of \"zebra\" in the two languages\n",
    "english_words = get_words(target)\n",
    "english_word_counts = Counter(english_words)\n",
    "print(\"English:%.15f%%\" % (english_word_counts['speaker'] / len(english_words) * 100)) \n",
    "print(\"English:%.15f%%\" % (english_word_counts['zebra'] / len(english_words) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Normal case\n",
      "P(house|the) is 0.002139909308605 \t P(S): 0.002139909308605\n",
      "P(rose|house) is 0.006329113924051 \t P(S): 0.000013543729801\n",
      "P('the house rose') is 0.000013543729801, lP: 0.000013543729801 \n",
      "-----\n",
      "P(says|it) is 0.001999200319872 \t P(S): 0.001999200319872\n",
      "P(that|says) is 0.529411764705882 \t P(S): 0.001058400169344\n",
      "P(this|that) is 0.046821985976024 \t P(S): 0.000049556397886\n",
      "P(should|this) is 0.003901560624250 \t P(S): 0.000000193347291\n",
      "P(be|should) is 0.295402298850575 \t P(S): 0.000000057115234\n",
      "P(done|be) is 0.016312594840668 \t P(S): 0.000000000931698\n",
      "P('it says that this should be done') is 0.000000000931698, lP: 0.000000000931698 \n",
      "----- Non exist\n",
      "P(house|the) is 0.002139909308605 \t P(S): 0.002139909308605\n",
      "P(rose|house) is 0.006329113924051 \t P(S): 0.000013543729801\n",
      "P(NNN|rose) is 0.000003808972415 \t P(S): 0.000000000051588\n",
      "P('the house rose NNN') is 0.000000000051588, lP: 0.000000000051588 \n",
      "----- over length\n",
      "P(will|you) is 0.052256532066508 \t P(S): 0.052256532066508\n",
      "P(be|will) is 0.251017639077341 \t P(S): 0.013117311305704\n",
      "P(aware|be) is 0.004172989377845 \t P(S): 0.000054738400745\n",
      "P(from|aware) is 0.025974025974026 \t P(S): 0.000001421776643\n",
      "P(the|from) is 0.373546511627907 \t P(S): 0.000000531099705\n",
      "P(press|the) is 0.000509502216335 \t P(S): 0.000000000270596\n",
      "P(and|press) is 0.150000000000000 \t P(S): 0.000000000040589\n",
      "P(television|and) is 0.000138619351261 \t P(S): 0.000000000000006\n",
      "P(that|television) is 0.125000000000000 \t P(S): 0.000000000000001\n",
      "P(there|that) is 0.028726532458720 \t P(S): 0.000000000000000\n",
      "P(have|there) is 0.013647642679901 \t P(S): 0.000000000000000\n",
      "P(been|have) is 0.115221987315011 \t P(S): 0.000000000000000\n",
      "P(a|been) is 0.029914529914530 \t P(S): 0.000000000000000\n",
      "P(number|a) is 0.016180492251595 \t P(S): 0.000000000000000\n",
      "P(of|number) is 0.924657534246575 \t P(S): 0.000000000000000\n",
      "P(bomb|of) is 0.000104887770086 \t P(S): 0.000000000000000\n",
      "P(explosions|bomb) is 0.200000000000000 \t P(S): 0.000000000000000\n",
      "P(and|explosions) is 1.000000000000000 \t P(S): 0.000000000000000\n",
      "P(killings|and) is 0.000138619351261 \t P(S): 0.000000000000000\n",
      "P(in|killings) is 0.500000000000000 \t P(S): 0.000000000000000\n",
      "P(sri|in) is 0.000484105212199 \t P(S): 0.000000000000000\n",
      "P(lanka|sri) is 0.750000000000000 \t P(S): 0.000000000000000\n",
      "len(122) ,P('you will b ...') is 0.000000000000000, lP: 0.000000000000000 \n"
     ]
    }
   ],
   "source": [
    "# bigram language model\n",
    "\n",
    "from math import log,exp\n",
    "\n",
    "class BigramLanguageModel:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def train(self, words):\n",
    "        self.bigrams = Counter(zip(words, words[1:]))\n",
    "        self.unigrams = Counter(words)\n",
    "        self.total = len(words)\n",
    "        \n",
    "    def probability_w(self, word1, word2):\n",
    "        if self.unigrams[word1] == 0 \\\n",
    "            or self.unigrams[word2] ==0 \\\n",
    "            or self.bigrams[(word1,word2)] == 0:\n",
    "            # if word non exist\n",
    "            return 1.0 / (self.total + 1)\n",
    "        return self.bigrams[(word1,word2)]/self.unigrams[word1]\n",
    "    \n",
    "    def predict(self, words: list):\n",
    "        # length of words should be at least 1\n",
    "        if len(words) == 0: return 1\n",
    "        Ps=1.0 # the probability_w (words[0]|<start>) is 1\n",
    "        for i in range(1,len(words)):\n",
    "            pt = self.probability_w(words[i-1],words[i])\n",
    "            Ps = Ps* pt\n",
    "            print(\"P(%s|%s) is %.15f \\t P(S): %.15f\" %(words[i],words[i-1], pt, Ps))\n",
    "            # print(\"\\t (%s, %s):%d, (%s): %d\" %(words[i-1],words[i],\n",
    "            #                                    self.bigrams[(words[i-1],words[i])], \n",
    "            #                                    words[i-1],\n",
    "            #                                    self.unigrams[words[i-1]]))\n",
    "        return Ps\n",
    "    \n",
    "    def predict_log(self, words: list):\n",
    "        # length of words should be at least 1\n",
    "        if len(words) == 0: return 1\n",
    "        Ps=0.0 # the probability_w (words[0]|<start>) is 1\n",
    "        for i in range(1,len(words)):\n",
    "            pt = log(self.probability_w(words[i-1],words[i]))\n",
    "            Ps = Ps + pt\n",
    "            # print(\"P(%s|%s) is %.15f \\t P(S): %.15f\" %(words[i],words[i-1], pt, Ps))\n",
    "            # print(\"\\t (%s, %s):%d, (%s): %d\" %(words[i-1],words[i],\n",
    "            #                                    self.bigrams[(words[i-1],words[i])], \n",
    "            #                                    words[i-1],\n",
    "            #                                    self.unigrams[words[i-1]]))\n",
    "        return exp(Ps)\n",
    "\n",
    "En_bigramM = BigramLanguageModel()\n",
    "En_bigramM.train(english_words)\n",
    "\n",
    "\n",
    "\n",
    "# print(english_words[:100])\n",
    "\n",
    "# debug\n",
    "# En_bigamM.probability_w('please','open') \n",
    "# En_bigamM.bigrams.most_common(10)\n",
    "# \n",
    "\n",
    "# case 1 normal short sentence \"the house rose\", from dataset\n",
    "print(\"----- Normal case\")\n",
    "sentence = \"the house rose\"\n",
    "print(\"P('%s') is %.15f, lP: %.15f \" % (sentence, \n",
    "                En_bigramM.predict(sentence.split(' ')),\n",
    "                En_bigramM.predict_log(sentence.split(' '))))\n",
    "print(\"-----\")\n",
    "sentence = \"it says that this should be done\"\n",
    "print(\"P('%s') is %.15f, lP: %.15f \" % (sentence, \n",
    "                En_bigramM.predict(sentence.split(' ')),\n",
    "                En_bigramM.predict_log(sentence.split(' '))))\n",
    "\n",
    "# case 2 words non exist\n",
    "print(\"----- Non exist\")\n",
    "sentence = \"the house rose NNN\"\n",
    "print(\"P('%s') is %.15f, lP: %.15f \" % (sentence, \n",
    "                En_bigramM.predict(sentence.split(' ')),\n",
    "                En_bigramM.predict_log(sentence.split(' '))))\n",
    "\n",
    "# case 3 words super length\n",
    "print(\"----- over length\")\n",
    "sentence = \"you will be aware from the press and television that there have been a number of bomb explosions and killings in sri lanka\"\n",
    "print(\"len(%d) ,P('%s ...') is %.15f, lP: %.15f \" % (len(sentence),\n",
    "                sentence[:10],\n",
    "                En_bigramM.predict(sentence.split(' ')),\n",
    "                En_bigramM.predict_log(sentence.split(' '))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source: je déclare, target: i declare \n",
      "source_sentences: 10001, target_sentences: 10001\n",
      "source_words: 300843, target_words: 262537\n",
      "Start iteration\n",
      "done iteration 1\n",
      "european \t [('apos', 0.061303760197027324), ('de', 0.05244150318322246), ('la', 0.03334956717801669), ('l', 0.030437652799466627), ('le', 0.02237151005233982), ('et', 0.021096483405532492), ('européenne', 0.017470748371094965), ('les', 0.01715472257961809), ('à', 0.01708830184441074), ('des', 0.015994861187823436)]\n",
      "\n",
      "done iteration 2\n",
      "european \t [('apos', 0.09539136956634232), ('européenne', 0.09392552466021101), ('de', 0.07910965058152969), ('l', 0.0550446241577374), ('européen', 0.049292267375503664), ('la', 0.04659814082371237), ('union', 0.036461485418055536), ('le', 0.03263639237724352), ('et', 0.02765991839613385), ('à', 0.021652868040345004)]\n",
      "\n",
      "done iteration 3\n",
      "european \t [('européenne', 0.2144609887065137), ('européen', 0.11587607714485212), ('apos', 0.09306539526957101), ('de', 0.07569906038711674), ('l', 0.061983882320843446), ('la', 0.041134055663651664), ('union', 0.04055017862690694), ('le', 0.030706720984867507), ('et', 0.022787949393605508), ('à', 0.018180422754475525)]\n",
      "\n",
      "done iteration 4\n",
      "european \t [('européenne', 0.306150113478447), ('européen', 0.17405915522503024), ('apos', 0.084146689888012), ('de', 0.06641487294231717), ('l', 0.06290442802805071), ('la', 0.033068606138887215), ('union', 0.028746834006884152), ('le', 0.02656430349451111), ('et', 0.01654801526791893), ('à', 0.013971209254237033)]\n",
      "\n",
      "done iteration 5\n",
      "european \t [('européenne', 0.36268721400795056), ('européen', 0.21282588362191968), ('apos', 0.07784650151642765), ('l', 0.06377039948392663), ('de', 0.05911939082542544), ('la', 0.026770783425234774), ('le', 0.0231826820064307), ('union', 0.018192240037488), ('et', 0.011906180126082041), ('d', 0.011371156433177815)]\n",
      "\n",
      "done iteration 6\n",
      "european \t [('européenne', 0.3979130265560956), ('européen', 0.23696053481701415), ('apos', 0.07430226847966766), ('l', 0.06545374595132401), ('de', 0.05402999399170392), ('la', 0.022095433688198805), ('le', 0.02064188222790495), ('union', 0.011205673895162839), ('d', 0.009554031276812888), ('au', 0.008899151012013615)]\n",
      "\n",
      "done iteration 7\n",
      "european \t [('européenne', 0.42105886900474443), ('européen', 0.252167408378497), ('apos', 0.07252773724248936), ('l', 0.06767291363955714), ('de', 0.050371793212161196), ('le', 0.018661729095987067), ('la', 0.01848988547448643), ('au', 0.008317960398131756), ('d', 0.008200144589100242), ('à', 0.0071633863772754824)]\n",
      "\n",
      "done iteration 8\n",
      "european \t [('européenne', 0.436988847167001), ('européen', 0.26218524564377466), ('apos', 0.07179455071701304), ('l', 0.07009049118493589), ('de', 0.04757116124449875), ('le', 0.01703625586720617), ('la', 0.015593837823686951), ('au', 0.007885520311552477), ('d', 0.007142215994338287), ('à', 0.005973867596465537)]\n",
      "\n",
      "done iteration 9\n",
      "european \t [('européenne', 0.44833388585558154), ('européen', 0.2691188095016427), ('l', 0.07247892966824859), ('apos', 0.07166274392823625), ('de', 0.045287802147563325), ('le', 0.015642331628040247), ('la', 0.013202022305243425), ('au', 0.007552612826861236), ('d', 0.006281547633682831), ('à', 0.0050417602337519025)]\n",
      "\n",
      "done iteration 10\n",
      "european \t [('européenne', 0.45667425053645194), ('européen', 0.2741629265913141), ('l', 0.07471662069570735), ('apos', 0.07188564808808141), ('de', 0.043334516968978276), ('le', 0.014410650699095837), ('la', 0.011195968393243307), ('au', 0.007287180294041534), ('d', 0.0055603962685557284), ('à', 0.004293034231628162)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# translate model IBM1\n",
    "\n",
    "def print_n_words(english_words, t):\n",
    "    for word in english_words:\n",
    "        sorted_t = sorted(t[word].items(), key=lambda x: x[1], reverse=True)\n",
    "        print(\"%s \\t %s\" % (word, sorted_t[0:10]))\n",
    "\n",
    "class IBM1():\n",
    "    def __init__(self):\n",
    "        self.t = {}\n",
    "                  \n",
    "    def train(self, source_txt, target_txt, n_iter=5):\n",
    "         \n",
    "        source_sentences = source_txt.split('\\n')\n",
    "        target_sentences = target_txt.split('\\n')\n",
    "        print(\"source_sentences: %d, target_sentences: %d\" % (len(source_sentences), len(target_sentences)))  \n",
    "        # get words\n",
    "        sa_words = re.findall(r'\\w+', source_txt.lower())\n",
    "        ta_words = re.findall(r'\\w+', target_txt.lower())\n",
    "        print(\"source_words: %d, target_words: %d\" % (len(sa_words), len(ta_words)))\n",
    "                \n",
    "        # self.t, c_e_f, c_e = self.init_t_c(s_words, t_words)\n",
    "        c_e_f={}\n",
    "        c_e={}\n",
    "        # start iteration\n",
    "        print(\"Start iteration\")\n",
    "        def single_iteration():\n",
    "            # set c(e -> f) and c(e) to 0\n",
    "            for key in c_e.keys():\n",
    "                c_e[key] = 0.0\n",
    "            for key in c_e_f.keys():\n",
    "                c_e_f[key] = 0.0\n",
    "            # update c(e -> f) and c(e)\n",
    "            for k in range(len(source_sentences)):\n",
    "                s_sentences = source_sentences[k]\n",
    "                t_sentences = target_sentences[k]\n",
    "                \n",
    "                s_words = re.findall(r'\\w+', s_sentences)\n",
    "                t_words = re.findall(r'\\w+', t_sentences)\n",
    "                \n",
    "                for i in range(len(s_words)):\n",
    "                    f = s_words[i]\n",
    "                    esum = 0.0\n",
    "                    for j in range(len(t_words)):\n",
    "                        e = t_words[j]\n",
    "                        if e not in self.t:\n",
    "                            #  initialize t(e|f) uniformly\n",
    "                            self.t[e] = {f: 1.0}\n",
    "                        if f not in self.t[e]:\n",
    "                            self.t[e][f] = 1.0\n",
    "                        esum += self.t[e][f]\n",
    "                        \n",
    "                    for j in range(len(t_words)):\n",
    "                        e = t_words[j]\n",
    "                        theta = self.t[e][f]/esum\n",
    "                        if (e,f) not in c_e_f:\n",
    "                            c_e_f[(e,f)] = 0.0\n",
    "                        c_e_f[(e,f)] += theta\n",
    "                        if e not in c_e:\n",
    "                            c_e[e] = 0.0\n",
    "                        c_e[e] += theta\n",
    "                # update t(e|f)\n",
    "            # print(\"update t(e|f)\")\n",
    "            for e in self.t.keys():\n",
    "                for f in self.t[e].keys():\n",
    "                    self.t[e][f] = c_e_f[(e,f)]/c_e[e]\n",
    "                    # print(\"t(%s|%s) = %.15f\" % (e,f, self.t[e][f]))\n",
    "                        \n",
    "        for i in range(n_iter):\n",
    "            single_iteration()\n",
    "            print(\"done iteration %d\" % (i+1)) \n",
    "            print_n_words([\"european\"], self.t)\n",
    "            print(\"\")\n",
    "   \n",
    "                            \n",
    "french_text = open(source, 'r').read()\n",
    "english_text = open(target, 'r').read()\n",
    "print(\"source: %s, target: %s\" % (french_text[:10], english_text[:10]))\n",
    "\n",
    "ibm1 = IBM1()\n",
    "# how many iteration we should run?\n",
    "ibm1.train(french_text, english_text, n_iter=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t(i|le) = 0.012032290049671\n",
      "t(declare|le) = 0.018263710936108\n",
      "t(the|le) = 0.101025707637215\n",
      "t(parliament|le) = 0.108474906036403\n",
      "t(president|le) = 0.189373841512907\n",
      "t(shall|le) = 0.198502316438452\n",
      "t(condemned|le) = 0.263157432090629\n",
      "t(tuned|le) = 0.274551740859560\n",
      "t(seniority|le) = 0.284148280174381\n",
      "t(kazakhstan|le) = 0.409060033869499\n",
      "t(cancers|le) = 0.440721727829749\n",
      "t(i|parlement) = 0.000000011055443\n",
      "t(declare|parlement) = 0.000037473669436\n",
      "t(parliament|parlement) = 0.789861707931134\n",
      "['cancers', 'parliament']\n"
     ]
    }
   ],
   "source": [
    "# Deconding \n",
    "# our goal is to find E = argmax P(E|F) = argmax P(F|E)P(E)\n",
    "# in the word alignment model:\n",
    "# is to find the best alignment E = argmax P(F,A|E,m), where A is the set of alignment, and \n",
    "# m is the length of F.\n",
    "# And P(F,A|E,m) = P(A|E,m)*P(F|A,E,m)\n",
    "# in IBM1 model, alignments are independent of each other and of the English words\n",
    "# so P(A|E,m) = \\prod_{i=1}^{m} q(a_i|E,m), and  we assume alignments are uniform\n",
    "# so P(A|E,m) can be ignored\n",
    "# thus argmax P(F,A|E,m) = P(F|A,E,m)\n",
    "# and assuming that word depends only on the english word it is aligned with\n",
    "# so P(F|A,E,m) = \\prod_{i=1}^{m} t(f_i|e_{a_i})\n",
    "# thus argmax P(F|A,E,m) = \\prod_{i=1}^{m} t(f_i|e_{a_i})\n",
    "\n",
    "# so it is a search problem, we can use greedy search, or beam search, or A* search\n",
    "\n",
    "def greedy_search(t, s_words):\n",
    "    t = t\n",
    "    s_words = s_words\n",
    "    t_words = []\n",
    "    for f in s_words:\n",
    "        e_p = 0 \n",
    "        e = None\n",
    "        for es in t.keys():\n",
    "            for fs in t[es]:\n",
    "                if fs ==f and t[es][fs] > e_p:\n",
    "                    print(\"t(%s|%s) = %.15f\" % (es,f, t[es][f]))\n",
    "                    e_p = t[es][f]\n",
    "                    e = es\n",
    "        if e is not None:\n",
    "            t_words.append(e)\n",
    "        else:\n",
    "            t_words.append(f)\n",
    "    return t_words\n",
    "\n",
    "# test greedy search\n",
    "\n",
    "sf = \"madame la présidente\"\n",
    "print(greedy_search(ibm1.t, re.findall(r'\\w+', sf)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
