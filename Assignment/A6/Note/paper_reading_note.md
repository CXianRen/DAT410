# Mastering the game of Go with deep neural networks and tree search

## Introduction
two networks were employed. one called **'value networks** is to evaluate **board positions**; 
and one called **'policy networks'** is to **select moves**. 

train:  supervised learning from human expert games ---> then reinforcment learning from games of self-play.


Effective search space can be reduced by two general principles:
+ First: the depth may be reduced by position evaluation: **truncating the search tree at state s** and **replacing the subtree below s by an aproximate value function v(s)**.
+ Second, the breadth of search may be reduced by **sampling actions from a policy p(a|s) that is a probability distribution over possible moves a in positions.** (???)

<!-- rollout: 试运行 -->


+ 3 policy networks
  + rollout policy: ?? for what, why need that?
  + SL policy: learn from data, to predict next step.
  + RL policy: based on SL policy, but is try to win as much as possible, instead of the accuracy.

+ value networks:
  + based on data generated by self-play with RL polocy, traning the value network to predict the expected outcome in positions


## Supervised learning of policy networks
<!-- 输入的特征 比想象的 多 -->
+ 13 layers
+ 30 million positions.
+ 57% acc with all input features, 55.7% acc with raw board position and move hisotry.
+ rollout policy: 2us vs 13 layers 3ms

## Reinforcement learning of policy networks
+ initialized with parameters of RL policy
+ reward function r(s):
  + t < T, r(s) = 0
  + t = T, r(s) = 1, z_t= +_r(s_T)

+ weights are updated at each time step t by stochastic gradient ascent. (???? how )

## Reinforcement learning of value networks
+ similar arch to the policy network, but outputs a single prediction
+ training with data  paris (s,z). that is why we need to save the self-play result. tp minimal 

## Saeching with policy and value networks
+  lookahead search 
+  the leaf node is evaluated in two ways:
   +  by the value network $v_{\theta}(s_L)$
   +  by the outcome $z_L$ of a radom rollout played out until terminal step T using fast rollout policy $p_{\pi}$
```math
  V(s_L) = (1-\lambda)v_{\theta}(s_L) + \lambda z_L
```

## Evaluating the playing strength of AlphaGo.

summary:

Mastering the Game of Go with Deep Neural Networks and Tree Search" introduces AlphaGo, a groundbreaking system for playing Go. AlphaGo employs two neural networks: a value network for evaluating board positions and a policy network for selecting moves. These networks are trained through supervised learning from expert games and reinforcement learning from self-play.

The system employs effective search space reduction techniques: truncating the search tree and replacing the subtree under the state "s" with the value network to reduce depth, and sampling actions from a probability distribution to reduce breadth. Additionally, AlphaGo utilizes three policy networks to select actions: the rollout policy, supervised learning (SL) policy, and reinforcement learning (RL) policy, each suited for different scenarios.

The value networks, trained alongside the policy networks, predict game outcomes based on data from self-play. During the search process, AlphaGo evaluates leaf nodes using both the value network's predictions and random rollouts, combining their evaluations for better decision-making. Overall, AlphaGo's integration of neural networks and search algorithms allows it to achieve unprecedented performance in the game of Go.